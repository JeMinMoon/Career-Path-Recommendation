import multiprocessing
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# 멀티프로세스를 사용하여 페이지를 처리하는 함수
def process_page(page_url, output_file):
    driver = webdriver.Chrome()
    driver.get(page_url)

    try:
        WebDriverWait(driver, 2).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'body')))
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')

        for j in range(10):
            list_num = str(j + 1)
            notice = soup.find(id='list' + list_num)
            links = notice.find('div', class_='cp-info-in').find('a', href=True)

            link_url = links['href']
            if not link_url.startswith('http'):
                link_url = 'https://www.work.go.kr' + link_url

            employmentNoticePage(link_url, output_file)  # employmentNoticePage 함수 호출

    finally:
        driver.quit()

# 직업 공고 페이지를 처리하는 함수
def employmentNoticePage(link_url, output_file):
    driver = webdriver.Chrome()
    driver.get(link_url)

    try:
        WebDriverWait(driver, 2).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'body')))
        new_page_source = driver.page_source
        new_soup = BeautifulSoup(new_page_source, 'html.parser')

        careers = new_soup.find_all('div', class_='careers-area')
        content_list = []
        for career in careers:
            career_text = career.get_text()
            # 필요한 정보 추출
            nessessary_text = career_text.split('모집요강')
            job_posting_title = [sentence.strip() for sentence in nessessary_text[0].split('\n') if sentence.strip()]  # 제목 추출

            if job_posting_title[0] == '조회수 :':
                content_list.append(job_posting_title[2])
            else:
                content_list.append(job_posting_title[1])

            data = []
            for idx in range(1, len(nessessary_text)):
                career_text = nessessary_text[idx].split('전형방법')[0].split('근무조건')[0].split('우대조건')[0].split('전형일정')[0].split('세부모집요강')[0].split('※')[0]
                print([career_text])

                # 필요한 부분 외 제거 작업
                # 줄바꿈 문자와 탭 문자를 제거하고 텍스트를 추출
                cleaned_t = career_text.split('              ')
                if len(cleaned_t) == 1:
                    cleaned_t = career_text.split('\t')[1:]
                else:
                    cleaned_t = cleaned_t[2:]
                    for idx in range(len(cleaned_t)):
                        cleaned_t[idx] = cleaned_t[idx].split('\n\n\n\n\n')
                        print(cleaned_t[idx])
                        if len(cleaned_t[idx]) == 1:
                            cleaned_t[idx] = cleaned_t[idx][0]
                        else:
                            cleaned_t[idx] = cleaned_t[idx][1]
                print(cleaned_t)
                processed_data = [item.replace('\n', ' ').strip() for item in cleaned_t if not item.startswith('\n') and item.replace('\n', '').strip()]
                
                print(processed_data)
                if len(processed_data) != 0:
                    print('ok')
                    data = data + processed_data
            content_list.append(data)

        # csv파일에 저장
        if len(content_list[1]) != 0:
            crawlling_result = pd.DataFrame({'job_title' : content_list[0], 'texts' : [', '.join(content_list[1])]})
        else:
            crawlling_result = pd.DataFrame({'job_title' : content_list[0], 'texts' : ['" "']})

        crawlling_result.to_csv(output_file, mode='a', header=False, index=False)
        print(crawlling_result)

    finally:
        driver.quit()

if __name__ == "__main__":
    #base_url = 'https://www.work.go.kr'
    num_processes = multiprocessing.cpu_count()  # 사용 가능한 CPU 코어 수
    print(num_processes)

    output_file = 'crawlling_result.csv' # 모든 정보를 하나의 csv 파일에 출력
    #새로운 csv파일 생성
    crawlling_result = pd.DataFrame(columns=['job_title', 'texts'])
    crawlling_result.to_csv(output_file, header=True, index=False)

    # 멀티프로세싱을 사용하여 각 페이지를 병렬로 처리
    with multiprocessing.Pool(num_processes) as pool:
        for i in range(3):
            page_num = i + 1
            url = f'https://www.work.go.kr/empInfo/empInfoSrch/list/dtlEmpSrchList.do?pageIndex={page_num}'
            # output_file = f'output_{page_num}.txt'  # 페이지별로 별도의 출력 파일 사용

            # 각 페이지를 병렬로 처리하는 프로세스 생성
            pool.apply_async(process_page, (url, output_file))

        pool.close()
        pool.join()
